{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f83e24e2-2a64-4a62-99ac-2ce125cb2159",
   "metadata": {},
   "source": [
    "# Cell 1: Install and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22cf1a2-85f7-4d9c-b74d-8161b3fd8ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/firstEnv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, Trainer, TrainingArguments\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fd5759-f77a-4566-9732-cf4684691a6f",
   "metadata": {},
   "source": [
    "# Cell 2: Set Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f58c032-0d04-4b0c-b603-a7096320c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4409a3-b65b-4451-b56a-b676e8f11940",
   "metadata": {},
   "source": [
    "# Cell 3: Data Directory and Label Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "996e1826-0a2e-4ca9-8934-0cc594d87650",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"  # Update this to your actual data directory\n",
    "art_styles = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "\n",
    "image_paths = []\n",
    "labels = []\n",
    "valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.gif')\n",
    "\n",
    "for label in art_styles:\n",
    "    label_dir = os.path.join(data_dir, label)\n",
    "    for img_name in os.listdir(label_dir):\n",
    "        if img_name.lower().endswith(valid_extensions):\n",
    "            image_paths.append(os.path.join(label_dir, img_name))\n",
    "            labels.append(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81485135-63c5-4a17-be47-b44c6527597a",
   "metadata": {},
   "source": [
    "# Cell 4: Data Splitting (Train, Val, Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc294d3f-626e-4f69-a62d-9b60084f1289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Training set size: 29750\n",
      "Validation set size: 6375\n",
      "Test set size: 6375\n"
     ]
    }
   ],
   "source": [
    "train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
    "    image_paths, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
    "    temp_paths, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    ")\n",
    "\n",
    "print(f\"Initial Training set size: {len(train_paths)}\")\n",
    "print(f\"Validation set size: {len(val_paths)}\")\n",
    "print(f\"Test set size: {len(test_paths)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c7eb6-a87b-4daf-ae84-d0d88ca482ab",
   "metadata": {},
   "source": [
    "# 5: Use a Smaller Subset of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e459cea-ec28-45aa-8c29-43424f20db58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training set size after style-based filtering: 25168\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "max_per_style = 3000\n",
    "style_to_indices = defaultdict(list)\n",
    "\n",
    "# Group training images by style\n",
    "for i, label in enumerate(train_labels):\n",
    "    style_to_indices[label].append(i)\n",
    "\n",
    "final_train_indices = []\n",
    "\n",
    "# For each style, take up to 2000 images\n",
    "for style, indices in style_to_indices.items():\n",
    "    if len(indices) > max_per_style:\n",
    "        chosen = random.sample(indices, max_per_style)\n",
    "    else:\n",
    "        chosen = indices\n",
    "    final_train_indices.extend(chosen)\n",
    "\n",
    "# Shuffle final indices to avoid any order bias\n",
    "random.shuffle(final_train_indices)\n",
    "\n",
    "train_paths_subset = [train_paths[i] for i in final_train_indices]\n",
    "train_labels_subset = [train_labels[i] for i in final_train_indices]\n",
    "\n",
    "print(f\"Final Training set size after style-based filtering: {len(train_paths_subset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6899823-5f81-4628-98bf-4c223316c4b2",
   "metadata": {},
   "source": [
    "# 6: Dataset Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "593af659-04a2-4ce0-ad64-514cedeb6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "class ArtStyleDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        unique_labels = list(set(labels))\n",
    "        self.label2id = {label: i for i, label in enumerate(sorted(unique_labels))}\n",
    "        self.id2label = {i: label for label, i in self.label2id.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        label_id = self.label2id[label]\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        processed = image_processor(image, return_tensors=\"pt\")\n",
    "        pixel_values = processed[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": label_id\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefd4e41-04cd-430f-b371-4a7eaad7d0da",
   "metadata": {},
   "source": [
    "# 7: Initialize Datasets with Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bfa0c79-ff9a-4f43-add5-550608e8f16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 13\n",
      "Sample from train_dataset: {'pixel_values': tensor([[[-0.1294, -0.1686, -0.1686,  ..., -0.2549, -0.3020, -0.2627],\n",
      "         [-0.1843, -0.1373, -0.1686,  ..., -0.2549, -0.3176, -0.3255],\n",
      "         [-0.1843, -0.1294, -0.1843,  ..., -0.2392, -0.3098, -0.3490],\n",
      "         ...,\n",
      "         [-0.4039, -0.3804, -0.3804,  ..., -0.3333, -0.3255, -0.2863],\n",
      "         [-0.3961, -0.3804, -0.3490,  ..., -0.3569, -0.3647, -0.3255],\n",
      "         [-0.4196, -0.3569, -0.3333,  ..., -0.3647, -0.3569, -0.2863]],\n",
      "\n",
      "        [[-0.1373, -0.1686, -0.1686,  ..., -0.2157, -0.2549, -0.2157],\n",
      "         [-0.2000, -0.1294, -0.1608,  ..., -0.2157, -0.2784, -0.2784],\n",
      "         [-0.1922, -0.1294, -0.1765,  ..., -0.2000, -0.2706, -0.3020],\n",
      "         ...,\n",
      "         [-0.4039, -0.3961, -0.4118,  ..., -0.4118, -0.4039, -0.3804],\n",
      "         [-0.4118, -0.4039, -0.3882,  ..., -0.4275, -0.4510, -0.4196],\n",
      "         [-0.4431, -0.3961, -0.3882,  ..., -0.4275, -0.4431, -0.3804]],\n",
      "\n",
      "        [[-0.2314, -0.2471, -0.2314,  ..., -0.2627, -0.3176, -0.3020],\n",
      "         [-0.2941, -0.2157, -0.2235,  ..., -0.2549, -0.3333, -0.3569],\n",
      "         [-0.2863, -0.2078, -0.2392,  ..., -0.2235, -0.3098, -0.3569],\n",
      "         ...,\n",
      "         [-0.4667, -0.4745, -0.4902,  ..., -0.4824, -0.4588, -0.4667],\n",
      "         [-0.4824, -0.4980, -0.4745,  ..., -0.5059, -0.5137, -0.5137],\n",
      "         [-0.5294, -0.4980, -0.4824,  ..., -0.5216, -0.5137, -0.4902]]]), 'labels': 1}\n",
      "Final Training set size: 25168\n",
      "Validation set size: 6375\n",
      "Test set size: 6375\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ArtStyleDataset(train_paths_subset, train_labels_subset)\n",
    "val_dataset = ArtStyleDataset(val_paths, val_labels)\n",
    "test_dataset = ArtStyleDataset(test_paths, test_labels)\n",
    "\n",
    "print(\"Number of classes:\", len(set(labels)))\n",
    "print(\"Sample from train_dataset:\", train_dataset[0])\n",
    "print(f\"Final Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a5772-f227-41ef-8d69-256e391d27aa",
   "metadata": {},
   "source": [
    "# 8: Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d563543-8cfb-43c5-83c0-a8c77a4b0065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(set(labels))\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: l for i, l in enumerate(sorted(set(labels)))},\n",
    "    label2id={l: i for i, l in enumerate(sorted(set(labels)))}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0525b43d-4010-4a26-b60b-705e9b60561f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.vit.modeling_vit.ViTForImageClassification'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92821d75-5019-4484-88b0-604ddfecfca9",
   "metadata": {},
   "source": [
    "# 9: Define Compute Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9987c8c-2b72-456e-97d7-39e310e4f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076d4d4e-6a2f-4830-a7d9-855b7376eeca",
   "metadata": {},
   "source": [
    "# 10: Training Arguments and Trainer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcb52153-b3fa-48e5-96a2-755ca21176d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/firstEnv/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/firstEnv/lib/python3.9/site-packages/transformers/training_args.py:1540: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3, \n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    no_cuda=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "391bb5ce-5352-48ed-8eb4-69c2c00c3102",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ef9f6-8f58-467f-be52-ab259687f0c6",
   "metadata": {},
   "source": [
    "# 11: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6e80a5a-641a-427c-bf8c-4e4a9404671e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18876' max='18876' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18876/18876 4:29:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.781400</td>\n",
       "      <td>0.971900</td>\n",
       "      <td>0.696000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.531400</td>\n",
       "      <td>0.873097</td>\n",
       "      <td>0.759216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.681600</td>\n",
       "      <td>0.938031</td>\n",
       "      <td>0.775059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/firstEnv/lib/python3.9/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/firstEnv/lib/python3.9/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/firstEnv/lib/python3.9/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18876, training_loss=0.7345903323302931, metrics={'train_runtime': 16151.9344, 'train_samples_per_second': 4.675, 'train_steps_per_second': 1.169, 'total_flos': 5.851532026727203e+18, 'train_loss': 0.7345903323302931, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d02adc9-bb2b-48eb-8067-81ae4c06a186",
   "metadata": {},
   "source": [
    "# 12: Validation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "695f8d29-5597-43b7-afa8-e53f024fc5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1594' max='1594' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1594/1594 09:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/firstEnv/lib/python3.9/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results: {'eval_loss': 0.8730968832969666, 'eval_accuracy': 0.7592156862745097, 'eval_runtime': 561.2769, 'eval_samples_per_second': 11.358, 'eval_steps_per_second': 2.84, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"Validation Results:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1ca5cf-7d4a-42ad-b864-458aa26a55a2",
   "metadata": {},
   "source": [
    "# 13: Test and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff544d8-e8d2-4109-bd57-0ec3661bf386",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = trainer.predict(test_dataset)\n",
    "print(\"Test Results:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bd24161-37fd-4f27-8d8e-d8125e44af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./vit_finetune\")\n",
    "\n",
    "image_processor.save_pretrained(\"./vit_finetune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822456bc-e9be-43d5-84ab-d51ffdb43217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
